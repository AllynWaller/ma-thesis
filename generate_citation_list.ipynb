{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anabasis.perseus-grc2.xml', 'Apology.perseus-grc2.xml', 'Economicus.perseus-grc2.xml', 'Hellenica.perseus-grc2.xml', 'Herodotus.xml', 'Hiero.perseus-grc2.xml', 'Memorabilia.perseus-grc2.xml', 'Symposium.perseus-grc2.xml', 'Thucydides.xml', 'tlg0010.tlg001.perseus-grc2.xml', 'tlg0010.tlg002.perseus-grc2.xml', 'tlg0010.tlg003.perseus-grc2.xml', 'tlg0010.tlg004.perseus-grc2.xml', 'tlg0010.tlg005.perseus-grc2.xml', 'tlg0010.tlg006.perseus-grc2.xml', 'tlg0010.tlg007.perseus-grc2.xml', 'tlg0010.tlg008.perseus-grc2.xml', 'tlg0010.tlg009.perseus-grc2.xml', 'tlg0010.tlg010.perseus-grc2.xml', 'tlg0010.tlg011.perseus-grc2.xml', 'tlg0010.tlg012.perseus-grc2.xml', 'tlg0010.tlg013.perseus-grc2.xml', 'tlg0010.tlg014.perseus-grc2.xml', 'tlg0010.tlg015.perseus-grc2.xml', 'tlg0010.tlg016.perseus-grc2.xml', 'tlg0010.tlg017.perseus-grc2.xml', 'tlg0010.tlg018.perseus-grc2.xml', 'tlg0010.tlg019.perseus-grc2.xml', 'tlg0010.tlg020.perseus-grc2.xml', 'tlg0010.tlg021.perseus-grc2.xml', 'tlg0010.tlg022.perseus-grc2.xml', 'tlg0010.tlg023.perseus-grc2.xml', 'tlg0010.tlg024.perseus-grc2.xml', 'tlg0010.tlg025.perseus-grc2.xml', 'tlg0010.tlg026.perseus-grc3.xml', 'tlg0010.tlg027.perseus-grc2.xml', 'tlg0010.tlg028.perseus-grc2.xml', 'tlg0010.tlg029.perseus-grc2.xml', 'tlg0010.tlg030.perseus-grc2.xml', 'tlg0014.tlg001.perseus-grc2.xml', 'tlg0014.tlg002.perseus-grc2.xml', 'tlg0014.tlg003.perseus-grc2.xml', 'tlg0014.tlg004.perseus-grc2.xml', 'tlg0014.tlg005.perseus-grc2.xml', 'tlg0014.tlg006.perseus-grc2.xml', 'tlg0014.tlg007.perseus-grc2.xml', 'tlg0014.tlg008.perseus-grc2.xml', 'tlg0014.tlg009.perseus-grc2.xml', 'tlg0014.tlg010.perseus-grc2.xml', 'tlg0014.tlg011.perseus-grc2.xml', 'tlg0014.tlg012.perseus-grc2.xml', 'tlg0014.tlg013.perseus-grc2.xml', 'tlg0014.tlg014.perseus-grc2.xml', 'tlg0014.tlg015.perseus-grc2.xml', 'tlg0014.tlg016.perseus-grc2.xml', 'tlg0014.tlg017.perseus-grc2.xml', 'tlg0014.tlg018.perseus-grc2.xml', 'tlg0014.tlg019.perseus-grc2.xml', 'tlg0014.tlg020.perseus-grc2.xml', 'tlg0014.tlg021.perseus-grc2.xml', 'tlg0014.tlg022.perseus-grc2.xml', 'tlg0014.tlg023.perseus-grc2.xml', 'tlg0014.tlg024.perseus-grc2.xml', 'tlg0014.tlg025.perseus-grc2.xml', 'tlg0014.tlg026.perseus-grc2.xml', 'tlg0014.tlg027.perseus-grc2.xml', 'tlg0014.tlg028.perseus-grc2.xml', 'tlg0014.tlg029.perseus-grc2.xml', 'tlg0014.tlg030.perseus-grc2.xml', 'tlg0014.tlg031.perseus-grc2.xml', 'tlg0014.tlg032.perseus-grc2.xml', 'tlg0014.tlg033.perseus-grc2.xml', 'tlg0014.tlg034.perseus-grc2.xml', 'tlg0014.tlg035.perseus-grc2.xml', 'tlg0014.tlg036.perseus-grc2.xml', 'tlg0014.tlg037.perseus-grc2.xml', 'tlg0014.tlg038.perseus-grc2.xml', 'tlg0014.tlg039.perseus-grc2.xml', 'tlg0014.tlg040.perseus-grc2.xml', 'tlg0014.tlg041.perseus-grc2.xml', 'tlg0014.tlg042.perseus-grc2.xml', 'tlg0014.tlg043.perseus-grc2.xml', 'tlg0014.tlg044.perseus-grc2.xml', 'tlg0014.tlg045.perseus-grc2.xml', 'tlg0014.tlg046.perseus-grc2.xml', 'tlg0014.tlg047.perseus-grc2.xml', 'tlg0014.tlg048.perseus-grc2.xml', 'tlg0014.tlg049.perseus-grc2.xml', 'tlg0014.tlg050.perseus-grc2.xml', 'tlg0014.tlg051.perseus-grc2.xml', 'tlg0014.tlg052.perseus-grc2.xml', 'tlg0014.tlg053.perseus-grc2.xml', 'tlg0014.tlg054.perseus-grc2.xml', 'tlg0014.tlg055.perseus-grc2.xml', 'tlg0014.tlg056.perseus-grc2.xml', 'tlg0014.tlg057.perseus-grc2.xml', 'tlg0014.tlg058.perseus-grc2.xml', 'tlg0014.tlg059.perseus-grc2.xml', 'tlg0014.tlg060.perseus-grc2.xml', 'tlg0014.tlg061.perseus-grc2.xml', 'tlg0014.tlg062.perseus-grc2.xml', 'tlg0014.tlg063.perseus-grc2.xml', 'tlg0540.tlg001.perseus-grc2.xml', 'tlg0540.tlg002.perseus-grc2.xml', 'tlg0540.tlg003.perseus-grc2.xml', 'tlg0540.tlg004.perseus-grc2.xml', 'tlg0540.tlg005.perseus-grc2.xml', 'tlg0540.tlg006.perseus-grc2.xml', 'tlg0540.tlg007.perseus-grc2.xml', 'tlg0540.tlg008.perseus-grc2.xml', 'tlg0540.tlg009.perseus-grc2.xml', 'tlg0540.tlg010.perseus-grc2.xml', 'tlg0540.tlg011.perseus-grc2.xml', 'tlg0540.tlg012.perseus-grc2.xml', 'tlg0540.tlg013.perseus-grc2.xml', 'tlg0540.tlg014.perseus-grc2.xml', 'tlg0540.tlg015.perseus-grc2.xml', 'tlg0540.tlg016.perseus-grc2.xml', 'tlg0540.tlg017.perseus-grc2.xml', 'tlg0540.tlg018.perseus-grc2.xml', 'tlg0540.tlg019.perseus-grc2.xml', 'tlg0540.tlg020.perseus-grc2.xml', 'tlg0540.tlg021.perseus-grc2.xml', 'tlg0540.tlg022.perseus-grc2.xml', 'tlg0540.tlg023.perseus-grc2.xml', 'tlg0540.tlg024.perseus-grc2.xml', 'tlg0540.tlg025.perseus-grc2.xml', 'tlg0540.tlg026.perseus-grc2.xml', 'tlg0540.tlg027.perseus-grc2.xml', 'tlg0540.tlg028.perseus-grc2.xml', 'tlg0540.tlg029.perseus-grc2.xml', 'tlg0540.tlg030.perseus-grc2.xml', 'tlg0540.tlg031.perseus-grc2.xml', 'tlg0540.tlg032.perseus-grc2.xml', 'tlg0540.tlg033.perseus-grc2.xml', 'tlg0540.tlg034.perseus-grc2.xml']\n"
     ]
    }
   ],
   "source": [
    "#Checks for which files are available for use\n",
    "path = \"raw_XML/Everything\"\n",
    "listOfFiles = os.listdir(path)\n",
    "print(listOfFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for work in listOfFiles:\n",
    "\n",
    "    file = open(os.path.join(path, work), \"r\", encoding=\"utf-8\")\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "    content = re.sub(r'<\\?[^>]*>', '', content)\n",
    "    content = re.sub(r'<TEI[^>]*>', '', content)\n",
    "    content = re.sub(r'[\\t|\\r]', '', content)\n",
    "    content = re.sub(r'[\\s]+', ' ', content)\n",
    "    content = re.sub(r'\\n', '', content)\n",
    "    content = re.sub(r'<teiHeader>.*<body>', '', content)\n",
    "    content = re.sub(r'>', '>\\n', content)\n",
    "\n",
    "    #print(content)\n",
    "    \n",
    "    #Strip out the extra tags (milestone, persname, q, placename, etc)\n",
    "\n",
    "    #matches tags, but not their contents\n",
    "    cleantext = re.sub(r'</?persName>', '', content)\n",
    "    cleaner = re.sub(r'</?placeName[^<]*>', '', cleantext)\n",
    "    cleaner = re.sub(r'<milestone[^/]*/>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?foreign[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?quote[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?gloss[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?title[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?date[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?surname[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?sic[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?corr[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?choice[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?said[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?q[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?body[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?text[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?TEI[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?del[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?add[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?emph[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?l[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?cit[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?term[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?sp[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?gap[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?hi[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?anchor[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</?num[^>]*>', '', cleaner)\n",
    "\n",
    "    tags = []\n",
    "    allowed = ['</p>', '</div>', '</note>', '</head>', '</bibl>']\n",
    "    tags = re.findall(r'</[^[>|/]*/?>', cleaner)\n",
    "    unique_tags = [] \n",
    "    [unique_tags.append(x) for x in tags if x not in unique_tags] \n",
    "    #print(unique_tags)\n",
    "    bad_tags = []\n",
    "    [bad_tags.append(x) for x in unique_tags if x not in allowed] \n",
    "    print(bad_tags)\n",
    "    \n",
    "    #Get rid of note tags and their content\n",
    "\n",
    "    cleaner = re.sub(r'<note[^>]*>[^<]*</note>', '', cleaner)\n",
    "    cleaner = re.sub(r'<head[^>]*>[^<]*</head>', '', cleaner)\n",
    "    cleaner = re.sub(r'<speaker[^>]*>[^<]*</speaker>', '', cleaner)\n",
    "    cleaner = re.sub(r'<bibl[^>]*>[^<]*</bibl>', '', cleaner)\n",
    "    cleaner = re.sub(r'<!--[^>]*-->', '', cleaner)\n",
    "\n",
    "#get rid of <p> and <div> for now. Will be useful for cross-validation later\n",
    "\n",
    "    cleaner = re.sub(r'</?p[^>]*>', '', cleaner)\n",
    "    #cleaner = re.sub(r'<div[^>]*>', '', cleaner)\n",
    "    cleaner = re.sub(r'</div>', r'@~', cleaner)\n",
    "\n",
    "#Get rid of extra whitespace\n",
    "    cleaner = re.sub(r'\\n', ' ', cleaner)\n",
    "    cleaner = re.sub(r'[\\s]+', ' ', cleaner)\n",
    "\n",
    "#Replace nested </div> tags\n",
    "    cleaner = re.sub(r'(@~ )+', r'@~ ', cleaner)\n",
    "\n",
    "#Replace temporary symbols with tab and return characters\n",
    "    cleaner = re.sub(r'@~', '\\n', cleaner)\n",
    "\n",
    "    #print(cleaner)\n",
    "\n",
    "    sections = cleaner.split('\\n')\n",
    "    #print(sections)\n",
    "    work = ''\n",
    "    book = 0\n",
    "    chapter = 0\n",
    "    section = 0\n",
    "    citation_list = []\n",
    "\n",
    "    for text in sections: \n",
    "        text.strip()\n",
    "        if text.count('type=\\\"edition\\\"') >= 1:\n",
    "            found = re.search(r'urn[^\\\"]*', text)\n",
    "            if found:\n",
    "                work = found.group(0)\n",
    "            \n",
    "            book = 0\n",
    "            chapter = 0\n",
    "            section = 0\n",
    "        \n",
    "        if text.count('subtype=\\\"book\\\"') >= 1:\n",
    "            book = book + 1\n",
    "            chapter = 0\n",
    "            section = 0\n",
    "        \n",
    "        if text.count('subtype=\\\"chapter\\\"') >= 1:\n",
    "            chapter = chapter + 1\n",
    "            section = 0\n",
    "        \n",
    "        if text.count('subtype=\\\"section\\\"') >= 1:\n",
    "            section = section + 1\n",
    "        \n",
    "        citation = work+'@'+str(book)+\".\"+str(chapter)+\".\"+str(section)\n",
    "        citation_list.append([citation, text])\n",
    "    \n",
    "    for item in citation_list:\n",
    "        item[1] = re.sub(r'<div[^>]*>', '', item[1])\n",
    "        item[1] = item[1].strip()\n",
    "    \n",
    "    #Put the clean text in a file\n",
    "    cleanfile = open('processed_text/citation_list.tsv', 'a', encoding=\"utf-8\") \n",
    "    for item in citation_list:\n",
    "        cleanfile.write(item[0]+ \"\\t\"+item[1]+\"\\n\")\n",
    "        \n",
    "    cleanfile.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "#This should spit out a bunch of empty lists (\"[]\"), one for each file. If there is anything in the lists, that \n",
    "#means there are tags that are not being removed properly. Figure out which file is the problem (by counting) and\n",
    "#look for the problem tags in it to see how they need to be removed. If they wrap around stuff that isn't original\n",
    "#text, they need to be deleted with their contents, like <note>, <head>, or <speaker> tags. Otherwise, they can\n",
    "#join the massive list above that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
